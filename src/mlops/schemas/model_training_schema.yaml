GridSearch:
  cv: 5
  scoring: "recall"
  n_jobs: -1
  verbose: 2
  refit: True
  return_train_score: False

FeatureSelection:
  method: "SelectKBest"
  param_grid:
    k: [5, 7, 9, 11]

LogisticRegression:
  param_grid:
    penalty: ["l1", "l2"]
    C: [0.001, 0.01, 0.1, 1, 10, 100]
    solver: ["liblinear", "saga", "lbfgs", "newton-cg"]
    max_iter: [100, 200, 500]
    class_weight: [null, "balanced"]

RandomForest:
  param_grid:
    n_estimators: [50, 100, 200, 250]
    max_depth: [None, 5, 10, 20, 50]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    max_features: ["sqrt", "log2"]
    bootstrap: [True, False]
    criterion: ["gini", "entropy"]
    class_weight: [null, "balanced", "balanced_subsample"]

XGBoost:
  param_grid:
    n_estimators: [50, 100, 200, 250]
    max_depth: [3, 5, 7, 10]
    learning_rate: [0.01, 0.05, 0.1, 0.2]
    subsample: [0.5, 0.7, 1.0]
    colsample_bytree: [0.5, 0.7, 1.0]
    gamma: [0, 0.1, 0.3, 0.5]
    reg_alpha: [0, 0.1, 0.5, 1]
    reg_lambda: [1, 1.5, 2]
    scale_pos_weight: [1, 5, 10]
    min_child_weight: [1, 3, 5]
